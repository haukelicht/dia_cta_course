{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualized embedding with transformer models illustrated\n",
    "\n",
    "In this notebook, we begin to peek under the hood of a BERT transformer model to understand how contextualized embedding work.\n",
    "We then also introduce a couple of potential use cases that leverage contextualized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/dia_cta_course/blob/main/notebooks/block2/day1/contextualized_embedding_explained.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # # shallow clone of current state of main branch \n",
    "    # !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/dia_cta_course.git\n",
    "\n",
    "    !pip install -q umap-learn~=0.5.9.post2 bertviz==1.4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import (\n",
    "    BertForMaskedLM, \n",
    "    BertModel, BertTokenizer\n",
    ")\n",
    "\n",
    "from bertviz import head_view\n",
    "from bertviz.transformers_neuron_view import BertModel as BertVizModel \n",
    "from bertviz.transformers_neuron_view import BertTokenizer as BertVizTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some terminology\n",
    "\n",
    "Here are the most important peices of the transformer model we will work wit:\n",
    "\n",
    "- `model()` -- this _is_ the model (has been pre-trained, we only use it to compute contextualized embeddings)\n",
    "- `tokenizer()` -- this is the tokenizer associated with the model that converts raw texts into sequences of token IDs that point tot the corresponding tokens' location in the model's input embedding matrix\n",
    "- `outputs` -- this is what the model _outputs_ when we process an input sentence through it\n",
    "-  `outputs.hidden_states` -- these are the contextualized embeddings prodcued at each layer\n",
    "-  `outputs.last_hidden_state` -- this is the contextualized embeddings at the _final_ layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to the `transformers` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the standard library to work with transformer models is `transformers`.\n",
    "It provides access to pre-trained transformers models through its [model hub]().\n",
    "The `transformers` library is developed and maintained by Hugging Face Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained models and tokenizers\n",
    "\n",
    "To use a pre-trained model for embedding texts, we need two things:\n",
    "\n",
    "1. the model's tokenizer\n",
    "1. and of course the model itself\n",
    "\n",
    "We use the model to process a text though its **layers** to obtain the text's **embedding**.\n",
    "But to be able to do this, we need to **tokenize** the text to convert it into number – because deep neural network can only process with numbers, not with raw text.\n",
    "\n",
    "Below we load a pre-trained BERT model, specifically \"bert-base-uncased\", which is a smallish version of BERT (hence 'base' instead of 'large') that does not distinguish between upper- and lowercase letters (hence 'uncased'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the model we want to load\n",
    "model_id = 'bert-base-uncased'\n",
    "\n",
    "# load the pre-trained model and tokenizer \n",
    "model = BertModel.from_pretrained(model_id)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "# NOTE: this will trigger downloading the model and tokenizer if you haven't done so before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some information about the model by looking at its configuration attribute (`config`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimensionality: 768\n",
      "number of layers: 12\n",
      "vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# let's get some important information about the model\n",
    "print('embedding dimensionality:', model.config.hidden_size)\n",
    "print('number of layers:', model.config.num_hidden_layers)\n",
    "print('vocabulary size:', model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# lets' have a look at the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the models first component is a `BertEmbeddings` module that contains\n",
    "    1. the initial word embedding layer\n",
    "    2. the positional embedding\n",
    "- after this we have the `BertEncoder` module that consists of 12 `BertLayer`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to get the initial word embeddings, we can access them like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 768, padding_idx=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01018257, -0.06154883, -0.02649689, -0.0420608 ,  0.00116716],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.embeddings.word_embeddings)\n",
    "\n",
    "# let's get the first five values of the first embedding\n",
    "model.embeddings.word_embeddings.weight[0][:5].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: \n",
    "\n",
    "- the layers are attributes of the `model` and they are organized and nested as can be seen when calling `print(model)` \n",
    "- we get the actual parameters of the model from a layer's \"weigths\" (weights is just the machine learning term for parameters)\n",
    "- weights are $n$-dimensional arrays (called \"tensors\" in `pytorch` etc.) and we can index them just like numpy arrays\n",
    "- we use `detach()` because the model and its weights (parameters) are tracked by the optimization algorithm, which we dont need when we only want to see the weight values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main reason we use BERT & Co. is to obtain contextualized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized embedding\n",
    "\n",
    "To illustrate how contextualized embedding works in transformers, we will first look at how embeddings of the same word differ if their context differs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take two sentences what contain the word \"bank\" but use it with different meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Today, I will hike along the bank of a river.\",\n",
    "    \"Today, I will open a new bank account and deposit some money.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the transformer embedding of the word \"bank\" in these two sentences, we need to follow three steps:\n",
    "\n",
    "1. tokenize the texts and convert tokens into tokens IDs (to look-up their input embeddings)\n",
    "2. process these inputs through the model\n",
    "3. locate the embedding of the focal word in the two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer converts the text into tokens and maps the tokens to token IDs\n",
    "\n",
    "Token IDs indicate tokens' locations in the tokenizers vocabulary and hence the model's input embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2651,  1010,  1045,  2097, 21857,  2247,  1996,  2924,  1997,\n",
       "          1037,  2314,  1012,   102,     0,     0],\n",
       "        [  101,  2651,  1010,  1045,  2097,  2330,  1037,  2047,  2924,  4070,\n",
       "          1998, 12816,  2070,  2769,  1012,   102]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"decode\" these token IDs into their tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'today',\n",
       " ',',\n",
       " 'i',\n",
       " 'will',\n",
       " 'hike',\n",
       " 'along',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'of',\n",
       " 'a',\n",
       " 'river',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "- the `[CLS]` token is a special token used to summarize the information in a sequence (e.g., for classification tasks)\n",
    "- the `[SEP]` token is the special \"separator\" token that indicates sequence boundaries\n",
    "- the `[PAD]` token is the special \"padding\" token that is appended to sequences that are shorter than the other sequences in a batch to make the input rectengular (e.g., all rows have an equal number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use the tokenizer to get the token ID of the focal word \n",
    "focal_word_id = tokenizer.convert_tokens_to_ids('bank')\n",
    "focal_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create maks that is true where input ID == focal word ID\n",
    "mask = inputs['input_ids'] == focal_word_id\n",
    "\n",
    "# show the positions of the focal word in the input IDs\n",
    "np.where(mask)[1] # NOTE: \"bank\" has the the same position in the sequence of words in both sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "-------------\n",
      "[CLS]\tFalse\n",
      "today\tFalse\n",
      ",\tFalse\n",
      "i\tFalse\n",
      "will\tFalse\n",
      "hike\tFalse\n",
      "along\tFalse\n",
      "the\tFalse\n",
      "bank\tTrue\n",
      "of\tFalse\n",
      "a\tFalse\n",
      "river\tFalse\n",
      ".\tFalse\n",
      "[SEP]\tFalse\n",
      "\n",
      "Sentence 2\n",
      "-------------\n",
      "[CLS]\tFalse\n",
      "today\tFalse\n",
      ",\tFalse\n",
      "i\tFalse\n",
      "will\tFalse\n",
      "open\tFalse\n",
      "a\tFalse\n",
      "new\tFalse\n",
      "bank\tTrue\n",
      "account\tFalse\n",
      "and\tFalse\n",
      "deposit\tFalse\n",
      "some\tFalse\n",
      "money\tFalse\n",
      ".\tFalse\n",
      "[SEP]\tFalse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put tokens and mask side by side\n",
    "for i, (iids, msk) in enumerate(zip(inputs['input_ids'], mask)):\n",
    "    print(\"Sentence\", i+1)\n",
    "    print(\"-\" * 13)\n",
    "    for t, m in zip(tokenizer.convert_ids_to_tokens(iids), msk):\n",
    "        print(t, bool(m), sep='\\t')\n",
    "        if t == '[SEP]':\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) embed (process through model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the intial emebdding of the focal word (\"bank\")\n",
    "model.embeddings.word_embeddings.weight[focal_word_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0191, -0.0646, -0.0913, -0.0776, -0.0253], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings.weight[focal_word_id][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** We use `torch.no_grad()` to disable gradient tracking, which is used for \"back propagation\" – the method used to optimize deep neural networks' parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'pooler_output', 'hidden_states']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "# list the object's attributes\n",
    "list(dict(outputs).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiden states are the embeddings after each layer\n",
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final embedding can be accessed like this: \n",
    "outputs.hidden_states[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the shape:\n",
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) get the words' contextualized embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final transformer embeddings of bank in different contexts\n",
    "embeddings = outputs.last_hidden_state[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0908, -0.6298,  0.1233,  ..., -0.5037, -1.1361,  0.4891],\n",
       "        [ 1.2851,  0.0947,  0.7091,  ..., -0.5602, -0.4731,  0.0220]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47205228]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute cosine similarity between the two embeddings\n",
    "cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see that the similarity of \"bank\"'s transformer embedding deepends on the model layer we look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embed. : 1.000\n",
      "    layer  1 : 0.711\n",
      "    layer  2 : 0.632\n",
      "    layer  3 : 0.508\n",
      "    layer  4 : 0.450\n",
      "    layer  5 : 0.426\n",
      "    layer  6 : 0.418\n",
      "    layer  7 : 0.409\n",
      "    layer  8 : 0.395\n",
      "    layer  9 : 0.374\n",
      "    layer 10 : 0.410\n",
      "    layer 11 : 0.467\n",
      "    layer 12 : 0.472\n"
     ]
    }
   ],
   "source": [
    "# iterate over all layers\n",
    "for i, layer in enumerate(outputs.hidden_states):\n",
    "    embeddings = layer[mask]\n",
    "    similarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\n",
    "    print(f'    layer {str(i).rjust(2)} :' if i>0 else 'input embed. :', f\"{similarity:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Notes_:** \n",
    "\n",
    "- If the word \"bank\" was _not_ in the same position in both sentences, we would already get a similarity < 1.0 at the input step because the word's [positional embedding](https://www.kaggle.com/code/lorentzyeung/positional-embeddings-clearly-explained) would differ.\n",
    "- If the focal word was _not_ as polysemious (i.e., its meaning would depend less on context), these similarity values would generally be higher. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia_cta_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
